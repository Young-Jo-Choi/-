{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jhAv0l3nBTO"
      },
      "source": [
        "# DER (dynamic expandable representation)\n",
        "참고 논문 : https://arxiv.org/pdf/2103.16788.pdf <br>\n",
        "참고 코드 : https://github.com/Rhyssiyan/DER-ClassIL.pytorch or https://github.com/G-U-N/PyCIL\n",
        "\n",
        "\n",
        "\n",
        "## Structure of DER\n",
        "\n",
        "<img src=\"https://github.com/Young-Jo-Choi/paper_study/assets/59189961/909d8c72-2c82-4c15-a56a-9be381a65c36\" alt=\"My Image\" width=2000>\n",
        "\n",
        "출처 : https://arxiv.org/pdf/2103.16788.pdf <br>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "r3U33j3-u4Hu"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import models, transforms, datasets\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "import math\n",
        "import copy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training paramters"
      ],
      "metadata": {
        "id": "HY73x_VwtZXg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "gjK93_B8sFqE"
      },
      "outputs": [],
      "source": [
        "EPSILON = 1e-8\n",
        "\n",
        "init_epoch = 201\n",
        "init_lr = 0.1\n",
        "init_milestones = [60, 120, 170]\n",
        "init_lr_decay = 0.1\n",
        "init_weight_decay = 0.0005\n",
        "\n",
        "epochs = 171\n",
        "lrate = 0.1\n",
        "milestones = [80, 120, 150]\n",
        "lrate_decay = 0.1\n",
        "batch_size = 128\n",
        "weight_decay = 2e-4\n",
        "num_workers = 8\n",
        "T = 2"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utility functions"
      ],
      "metadata": {
        "id": "DdLRXwuatcz7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "LJpukksWYgde"
      },
      "outputs": [],
      "source": [
        "# utils\n",
        "def tensor2numpy(x):\n",
        "    return x.cpu().data.numpy() if x.is_cuda else x.data.numpy()\n",
        "\n",
        "def _map_new_class_index(y, order):\n",
        "    return np.array(list(map(lambda x: order.index(x), y)))\n",
        "\n",
        "def count_parameters(model, trainable=False):\n",
        "    if trainable:\n",
        "        return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    return sum(p.numel() for p in model.parameters())\n",
        "\n",
        "def accuracy(y_pred, y_true, nb_old, increment=10):\n",
        "    assert len(y_pred) == len(y_true), \"Data length error.\"\n",
        "    all_acc = {}\n",
        "    all_acc[\"total\"] = np.around((y_pred == y_true).sum() * 100 / len(y_true), decimals=2)\n",
        "\n",
        "    # Grouped accuracy\n",
        "    for class_id in range(0, np.max(y_true), increment):\n",
        "        idxes = np.where(np.logical_and(y_true >= class_id, y_true < class_id + increment))[0]\n",
        "        label = \"{}-{}\".format(str(class_id).rjust(2, \"0\"), str(class_id + increment - 1).rjust(2, \"0\"))\n",
        "        all_acc[label] = np.around((y_pred[idxes] == y_true[idxes]).sum() * 100 / len(idxes), decimals=2)\n",
        "\n",
        "    # Old accuracy\n",
        "    idxes = np.where(y_true < nb_old)[0]\n",
        "    all_acc[\"old\"] = (0 if len(idxes) == 0\n",
        "                        else np.around((y_pred[idxes] == y_true[idxes]).sum() * 100 / len(idxes), decimals=2))\n",
        "\n",
        "    # New accuracy\n",
        "    idxes = np.where(y_true >= nb_old)[0]\n",
        "    all_acc[\"new\"] = np.around((y_pred[idxes] == y_true[idxes]).sum() * 100 / len(idxes), decimals=2)\n",
        "\n",
        "    return all_acc"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem Setup\n",
        "During the class incremental learning, the model observes a stream of class groups {$\\mathcal{Y}_t$} and their corresponding training data {$\\mathcal{D}_t$}. Particularly, the incoming dataset {$\\mathcal{D}_t$} at step $t$ has a form of ($x_i^t,y_i^t$) where $x_i^t$ is the input image and $y_i^t \\in \\mathcal{Y}_t$ is the label within the label set $\\mathcal{Y}_t$. The label space of the model is all seen categories $\\tilde{\\mathcal{Y}_t}=\\cup_{i=1}^t\\mathcal{Y}_i$ and the model is expected to predict well on classes in $\\tilde{\\mathcal{Y}_t}$.\n",
        "<br>\n",
        "\n",
        "\n",
        "Our method adopts the rehearsal strategy, which saves a part of data as the memory $\\mathcal{M}_t$ for future training."
      ],
      "metadata": {
        "id": "gmBC2onAxSlC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "lCG_RYq9q6mZ"
      },
      "outputs": [],
      "source": [
        "# cifar100 다운로드\n",
        "class iCIFAR100():\n",
        "    train_trsf = [\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ColorJitter(brightness=63 / 255),\n",
        "        transforms.ToTensor()\n",
        "    ]\n",
        "    test_trsf = [transforms.ToTensor()]\n",
        "    common_trsf = [transforms.Normalize(mean=(0.5071, 0.4867, 0.4408), std=(0.2675, 0.2565, 0.2761))]\n",
        "    class_order = np.arange(100).tolist()\n",
        "\n",
        "    def download_data(self):\n",
        "        train_dataset = datasets.cifar.CIFAR100(\"./data\", train=True, download=True)\n",
        "        test_dataset = datasets.cifar.CIFAR100(\"./data\", train=False, download=True)\n",
        "        self.train_data, self.train_targets = train_dataset.data, np.array(train_dataset.targets)\n",
        "        self.test_data, self.test_targets = test_dataset.data, np.array(test_dataset.targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "7VNOGBOdsFqF"
      },
      "outputs": [],
      "source": [
        "class DummyDataset(Dataset):\n",
        "    def __init__(self, images, labels, trsf):\n",
        "        assert len(images) == len(labels), \"Data size error!\"\n",
        "        self.images = images\n",
        "        self.labels = labels\n",
        "        self.trsf = trsf\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.trsf(Image.fromarray(self.images[idx]))\n",
        "        label = self.labels[idx]\n",
        "        return idx, image, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "qzR_5madY-sG"
      },
      "outputs": [],
      "source": [
        "# Incremental learning을 위해 데이터를 분할해 관리하는 class\n",
        "class DataManager(object):\n",
        "    def __init__(self, seed, init_cls, increment):\n",
        "        self._setup_data(seed)\n",
        "        assert init_cls <= len(self._class_order), \"No enough classes.\"\n",
        "        self._increments = [init_cls]\n",
        "        while sum(self._increments) + increment < len(self._class_order):\n",
        "            self._increments.append(increment)\n",
        "        offset = len(self._class_order) - sum(self._increments)\n",
        "        if offset > 0:\n",
        "            self._increments.append(offset)\n",
        "\n",
        "    @property\n",
        "    def nb_tasks(self):\n",
        "        return len(self._increments)\n",
        "\n",
        "    def get_task_size(self, task):\n",
        "        return self._increments[task]\n",
        "\n",
        "    def get_dataset(self, indices, source, mode, appendent=None, ret_data=False, m_rate=None):\n",
        "        if source == \"train\":\n",
        "            x, y = self._train_data, self._train_targets\n",
        "        elif source == \"test\":\n",
        "            x, y = self._test_data, self._test_targets\n",
        "        else:\n",
        "            raise ValueError(\"Unknown data source {}.\".format(source))\n",
        "\n",
        "        if mode == \"train\":\n",
        "            trsf = transforms.Compose([*self._train_trsf, *self._common_trsf])\n",
        "        elif mode == \"test\":\n",
        "            trsf = transforms.Compose([*self._test_trsf, *self._common_trsf])\n",
        "        else:\n",
        "            raise ValueError(\"Unknown mode {}.\".format(mode))\n",
        "\n",
        "        data, targets = [], []\n",
        "        for idx in indices:\n",
        "            class_data, class_targets = self._select(x, y, low_range=idx, high_range=idx + 1)\n",
        "            data.append(class_data)\n",
        "            targets.append(class_targets)\n",
        "\n",
        "        # rehearsal memory\n",
        "        if appendent is not None and len(appendent) != 0:\n",
        "            appendent_data, appendent_targets = appendent\n",
        "            data.append(appendent_data)\n",
        "            targets.append(appendent_targets)\n",
        "\n",
        "        data, targets = np.concatenate(data), np.concatenate(targets)\n",
        "        if ret_data:\n",
        "            return data, targets, DummyDataset(data, targets, trsf)\n",
        "        else:\n",
        "            return DummyDataset(data, targets, trsf)\n",
        "\n",
        "    def _setup_data(self, seed):\n",
        "        idata = iCIFAR100()\n",
        "        idata.download_data()\n",
        "        self._train_data, self._train_targets = idata.train_data, idata.train_targets\n",
        "        self._test_data, self._test_targets = idata.test_data, idata.test_targets\n",
        "        self._train_trsf = idata.train_trsf\n",
        "        self._test_trsf = idata.test_trsf\n",
        "        self._common_trsf = idata.common_trsf\n",
        "\n",
        "        # class ordering\n",
        "        order = [i for i in range(len(np.unique(self._train_targets)))]\n",
        "        np.random.seed(seed)\n",
        "        self._class_order = np.random.permutation(len(order)).tolist()\n",
        "\n",
        "        # Map indices\n",
        "        self._train_targets = _map_new_class_index(self._train_targets, self._class_order)\n",
        "        self._test_targets = _map_new_class_index(self._test_targets, self._class_order)\n",
        "\n",
        "    def _select(self, x, y, low_range, high_range):\n",
        "        idxes = np.where(np.logical_and(y >= low_range, y < high_range))[0]\n",
        "        if isinstance(x, np.ndarray):\n",
        "            x_return = x[idxes]\n",
        "        else:\n",
        "            x_return = []\n",
        "            for id in idxes:\n",
        "                x_return.append(x[id])\n",
        "        return x_return, y[idxes]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training process\n",
        "For the learning of step $t$, we decouple the learning process into two sequential stages as follows.\n",
        "\n",
        "1) Representation Learning Stage. To achieve better trade-off between stability and plasticity, we fix the previous feature representation and expand it with a new feature extractor trained on the incoming and memory data. We design an auxiliary loss on the novel extractor to promote it to learn diverse and discriminative features. To improve the model efficiency, we dynamically expand the representation according to the complexity of new classes via introducing a channel-level mask-based pruning method.\n",
        "\n",
        "2) Classifier Learning Stage. After the learning of representation, we retrain the classifier with currently available data $\\tilde{\\mathcal{D}}_t = \\mathcal{D}_t \\cup \\mathcal{M}_t$ at step $t$.\n",
        "\n",
        "$\\Phi_t$ : super-feature extractor <br>\n",
        "$\\mathcal{F}_t$ : feature extractor <br>\n",
        "$\\mathcal{H}_t$ : classifier"
      ],
      "metadata": {
        "id": "wBb352IXH1T4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Extractor : Resnet32\n",
        "class ResNetBasicblock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super().__init__()\n",
        "        self.conv_a = nn.Conv2d(inplanes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn_a = nn.BatchNorm2d(planes)\n",
        "        self.conv_b = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn_b = nn.BatchNorm2d(planes)\n",
        "        self.downsample = downsample\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        basicblock = self.conv_a(x)\n",
        "        basicblock = self.bn_a(basicblock)\n",
        "        basicblock = F.relu(basicblock, inplace=True)\n",
        "        basicblock = self.conv_b(basicblock)\n",
        "        basicblock = self.bn_b(basicblock)\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "        return F.relu(residual + basicblock, inplace=True)\n",
        "\n",
        "class DownsampleA(nn.Module):\n",
        "    def __init__(self, nIn, nOut, stride):\n",
        "        super().__init__()\n",
        "        assert stride == 2\n",
        "        self.avg = nn.AvgPool2d(kernel_size=1, stride=stride)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.avg(x)\n",
        "        return torch.cat((x, x.mul(0)), 1)\n",
        "\n",
        "class CifarResNet(nn.Module):\n",
        "    def __init__(self, block, depth, channels=3):\n",
        "        super().__init__()\n",
        "        assert (depth - 2) % 6 == 0, 'depth should be one of 20, 32, 44, 56, 110'\n",
        "        layer_blocks = (depth - 2) // 6\n",
        "        self.conv_1_3x3 = nn.Conv2d(channels, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn_1 = nn.BatchNorm2d(16)\n",
        "        self.inplanes = 16\n",
        "        self.stage_1 = self._make_layer(block, 16, layer_blocks, 1)\n",
        "        self.stage_2 = self._make_layer(block, 32, layer_blocks, 2)\n",
        "        self.stage_3 = self._make_layer(block, 64, layer_blocks, 2)\n",
        "        self.avgpool = nn.AvgPool2d(8)\n",
        "        self.out_dim = 64 * block.expansion\n",
        "        self.fc = nn.Linear(64*block.expansion, 10)\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.kaiming_normal_(m.weight)\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = DownsampleA(self.inplanes, planes * block.expansion, stride)\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_1_3x3(x)  # [bs, 16, 32, 32]\n",
        "        x = F.relu(self.bn_1(x), inplace=True)\n",
        "        x_1 = self.stage_1(x)    # [bs, 16, 32, 32]\n",
        "        x_2 = self.stage_2(x_1)  # [bs, 32, 16, 16]\n",
        "        x_3 = self.stage_3(x_2)  # [bs, 64, 8, 8]\n",
        "        pooled = self.avgpool(x_3)  # [bs, 64, 1, 1]\n",
        "        features = pooled.view(pooled.size(0), -1)  # [bs, 64]\n",
        "        return features\n",
        "\n",
        "    @property\n",
        "    def last_conv(self):\n",
        "        return self.stage_3[-1].conv_b\n",
        "\n",
        "def get_convnet():\n",
        "    return CifarResNet(ResNetBasicblock, 32)"
      ],
      "metadata": {
        "id": "mH062jm8Hy71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "At step $t$, our model is composed of a super-feature extractor $\\Phi_t$ and the classifier $\\mathcal{H}_t$. The super-feature extractor $\\Phi_t$ is built by expanding the feature extractor $\\Phi_{t-1}$ with a newly created feature extractor $\\mathcal{F}_t$. Specifically, given an image $x \\in \\hat{\\mathcal{D}}_t$, the feature $u$ extracted by $\\Phi_t$ is obtained by concatenation as follows\n",
        "$$u = \\Phi_{t}(x) = [\\Phi_{t-1}(x), \\mathcal{F}_t(x)] $$\n",
        "\n",
        "Here we reuse the previous $\\mathcal{F}_1, . . . , \\mathcal{F}_{t−1}$ and encourage the new extractor $\\mathcal{F}_t$ to learn only novel aspect of new classes. The feature $u$ is then fed into the classifier Ht to make prediction as follows\n",
        "$$p_{\\mathcal{H}_t}(y|x) = \\text{Softmax}(\\mathcal{H}_t(u))$$\n",
        "\n",
        "Then the prediction $\\hat{y} = \\text{argmax} p_{\\mathcal{H}_t}(y|x), \\hat{y} \\in \\hat{\\mathcal{Y}}_t$. The classifier is designed to match its new input and output dimensions for step $t$. The parameters of $\\mathcal{H}_t$ for the old features are inherited from $\\mathcal{H}_{t-1}$ to retain old knowledge and its newly added parameters are randomly initialized.\n",
        "\n",
        "To reduce catastrophic forgetting, we freeze the learned function $\\Phi_{t-1}$ at step $t$, as it captures the intrinsic structure of previous data."
      ],
      "metadata": {
        "id": "tn1XPMp5MULq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Mscz7zqzn_Eq"
      },
      "outputs": [],
      "source": [
        "# DER network\n",
        "class DERNet(nn.Module):\n",
        "    def __init__(self, args, pretrained):\n",
        "        super().__init__()\n",
        "        self.convnets = nn.ModuleList()\n",
        "        self.pretrained = pretrained\n",
        "        self.out_dim = None\n",
        "        self.fc = None\n",
        "        self.aux_fc = None\n",
        "        self.task_sizes = []\n",
        "        self.args = args\n",
        "\n",
        "    @property\n",
        "    def feature_dim(self):\n",
        "        if self.out_dim is None:\n",
        "            return 0\n",
        "        return self.out_dim * len(self.convnets)\n",
        "\n",
        "    def extract_vector(self, x):\n",
        "        features = [convnet(x) for convnet in self.convnets]\n",
        "        features = torch.cat(features, 1)\n",
        "        return features\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = [convnet(x) for convnet in self.convnets]\n",
        "        features = torch.cat(features, 1)\n",
        "        out = self.fc(features)\n",
        "        aux_logits = self.aux_fc(features[:, -self.out_dim :])\n",
        "        out_dict = {\"logits\" :out, \"aux_logits\": aux_logits, \"features\": features}\n",
        "        return out_dict\n",
        "\n",
        "    def update_fc(self, nb_classes):\n",
        "        # Feature extractor 구성\n",
        "        if len(self.convnets) == 0:\n",
        "            self.convnets.append(get_convnet())\n",
        "        else:\n",
        "            self.convnets.append(get_convnet())\n",
        "            self.convnets[-1].load_state_dict(self.convnets[-2].state_dict())\n",
        "\n",
        "        if self.out_dim is None:\n",
        "            self.out_dim = self.convnets[-1].out_dim\n",
        "        fc = self.generate_fc(self.feature_dim, nb_classes)\n",
        "        if self.fc is not None:\n",
        "            nb_output = self.fc.out_features\n",
        "            weight = copy.deepcopy(self.fc.weight.data)\n",
        "            bias = copy.deepcopy(self.fc.bias.data)\n",
        "            fc.weight.data[:nb_output, : self.feature_dim - self.out_dim] = weight\n",
        "            fc.bias.data[:nb_output] = bias\n",
        "        del self.fc\n",
        "        self.fc = fc\n",
        "\n",
        "        new_task_size = nb_classes - sum(self.task_sizes)\n",
        "        self.task_sizes.append(new_task_size)\n",
        "        self.aux_fc = self.generate_fc(self.out_dim, new_task_size + 1)\n",
        "\n",
        "    def generate_fc(self, in_dim, out_dim):\n",
        "        fc = nn.Linear(in_dim, out_dim)\n",
        "        return fc\n",
        "\n",
        "    def weight_align(self, increment):\n",
        "        weights = self.fc.weight.data\n",
        "        newnorm = torch.norm(weights[-increment:, :], p=2, dim=1)\n",
        "        oldnorm = torch.norm(weights[:-increment, :], p=2, dim=1)\n",
        "        meannew = torch.mean(newnorm)\n",
        "        meanold = torch.mean(oldnorm)\n",
        "        gamma = meanold / meannew\n",
        "        print(\"alignweights,gamma=\", gamma)\n",
        "        self.fc.weight.data[-increment:, :] *= gamma\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training loss\n",
        "We learn the model with cross-entropy loss on memory and incoming data as follows\n",
        "$$ \\mathcal{L}_{\\mathcal{H}_t}= {-1 \\over |\\tilde{\\mathcal{D}}_t|} \\sum_{i=1}^{\\hat{\\mathcal{D}}_t} log(p_{\\mathcal{H}_t}(y=y_i|x_i)) $$\n",
        "where $x_i$ is image and $y_i$ is the corresponding label.\n",
        "\n",
        "\n",
        "To enforce the network to learn the diverse and discriminative features for novel concepts, we further develop an auxiliary loss operating on the novel feature $\\mathcal{F}_t(x)$. Specifically, we introduce an auxiliary classifier $\\mathcal{H}_t^a$ , which predicts the probability $p_{\\mathcal{H}_t^a}(y|x)=\\text{Softmax}(\\mathcal{H}_t^a(\\mathcal{F}_t(x)))$. To encourage the network to learn features to discriminate between old and new concepts, the label space of $\\mathcal{H}_t^a$ is $|\\mathcal{Y}_t|+1$ including the new category set $\\mathcal{Y}_t$ and the other class by treating all old concepts as one category. Thusly, we introduce the auxiliary loss and obtain the expandable representation loss as follows\n",
        "$$\\mathcal{L}_{ER} = \\mathcal{L}_{\\mathcal{H}_t} + \\lambda_a \\mathcal{L}_{\\mathcal{H}_t^a}$$\n",
        "where $\\lambda_a$ is the hyper-parameter to control the effect of the auxiliary classifier. It is worth noting that $\\lambda_a =0$ for first step $t = 1$.\n"
      ],
      "metadata": {
        "id": "Q0SLBuXbPTa5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "os2xmqNlsFqG"
      },
      "outputs": [],
      "source": [
        "# Incremental learning을 위한 class\n",
        "class DERBase():\n",
        "    def __init__(self, args):\n",
        "        self.args = args\n",
        "        self._cur_task = -1\n",
        "        self._known_classes = 0\n",
        "        self._total_classes = 0\n",
        "        self._network = DERNet(args, False)\n",
        "        self._data_memory, self._targets_memory = np.array([]), np.array([])\n",
        "        self.topk = 5\n",
        "\n",
        "        self._memory_size = args[\"memory_size\"]\n",
        "        self._memory_per_class = args.get(\"memory_per_class\", None)\n",
        "        self._fixed_memory = args.get(\"fixed_memory\", False)\n",
        "        self._device = args[\"device\"][0]\n",
        "\n",
        "    def after_task(self):\n",
        "        self._known_classes = self._total_classes\n",
        "        print(\"Exemplar size: {}\".format(self.exemplar_size))\n",
        "\n",
        "    @property\n",
        "    def feature_dim(self):\n",
        "        if isinstance(self._network, nn.DataParallel):\n",
        "            return self._network.module.feature_dim\n",
        "        else:\n",
        "            return self._network.feature_dim\n",
        "\n",
        "    @property\n",
        "    def exemplar_size(self):\n",
        "        assert len(self._data_memory) == len(self._targets_memory), \"Exemplar size error.\"\n",
        "        return len(self._targets_memory)\n",
        "    def _get_memory(self):\n",
        "        if len(self._data_memory) == 0:\n",
        "            return None\n",
        "        else:\n",
        "            return (self._data_memory, self._targets_memory)\n",
        "\n",
        "    @property\n",
        "    def samples_per_class(self):\n",
        "        if self._fixed_memory:\n",
        "            return self._memory_per_class\n",
        "        else:\n",
        "            assert self._total_classes != 0, \"Total classes is 0\"\n",
        "            return self._memory_size // self._total_classes\n",
        "\n",
        "    def _compute_accuracy(self, model, loader):\n",
        "        model.eval()\n",
        "        correct, total = 0, 0\n",
        "        for i, (_, inputs, targets) in enumerate(loader):\n",
        "            inputs = inputs.to(self._device)\n",
        "            with torch.no_grad():\n",
        "                outputs = model(inputs)[\"logits\"]\n",
        "            predicts = torch.max(outputs, dim=1)[1]\n",
        "            correct += (predicts.cpu() == targets).sum()\n",
        "            total += len(targets)\n",
        "        return np.around(tensor2numpy(correct) * 100 / total, decimals=2)\n",
        "\n",
        "    def _reduce_exemplar(self, data_manager, m):\n",
        "        print(\"Reducing exemplars...({} per classes)\".format(m))\n",
        "        dummy_data, dummy_targets = copy.deepcopy(self._data_memory), copy.deepcopy(self._targets_memory)\n",
        "        self._class_means = np.zeros((self._total_classes, self.feature_dim))\n",
        "        self._data_memory, self._targets_memory = np.array([]), np.array([])\n",
        "\n",
        "        for class_idx in range(self._known_classes):\n",
        "            mask = np.where(dummy_targets == class_idx)[0]\n",
        "            dd, dt = dummy_data[mask][:m], dummy_targets[mask][:m]\n",
        "            self._data_memory = (\n",
        "                np.concatenate((self._data_memory, dd))\n",
        "                if len(self._data_memory) != 0\n",
        "                else dd\n",
        "            )\n",
        "            self._targets_memory = (\n",
        "                np.concatenate((self._targets_memory, dt))\n",
        "                if len(self._targets_memory) != 0\n",
        "                else dt\n",
        "            )\n",
        "\n",
        "            # Exemplar mean\n",
        "            idx_dataset = data_manager.get_dataset([], source=\"train\", mode=\"test\", appendent=(dd, dt))\n",
        "            idx_loader = DataLoader(idx_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "            vectors, _ = self._extract_vectors(idx_loader)\n",
        "            vectors = (vectors.T / (np.linalg.norm(vectors.T, axis=0) + EPSILON)).T\n",
        "            mean = np.mean(vectors, axis=0)\n",
        "            mean = mean / np.linalg.norm(mean)\n",
        "\n",
        "            self._class_means[class_idx, :] = mean\n",
        "\n",
        "    def _construct_exemplar(self, data_manager, m):\n",
        "        print(\"Constructing exemplars...({} per classes)\".format(m))\n",
        "        for class_idx in range(self._known_classes, self._total_classes):\n",
        "            data, targets, idx_dataset = data_manager.get_dataset(\n",
        "                np.arange(class_idx, class_idx + 1),\n",
        "                source=\"train\",\n",
        "                mode=\"test\",\n",
        "                ret_data=True,\n",
        "            )\n",
        "            idx_loader = DataLoader(idx_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "            vectors, _ = self._extract_vectors(idx_loader)\n",
        "            vectors = (vectors.T / (np.linalg.norm(vectors.T, axis=0) + EPSILON)).T\n",
        "            class_mean = np.mean(vectors, axis=0)\n",
        "\n",
        "            # Select\n",
        "            selected_exemplars = []\n",
        "            exemplar_vectors = []   # [n, feature_dim]\n",
        "            for k in range(1, m + 1):\n",
        "                S = np.sum(exemplar_vectors, axis=0)            # [feature_dim] sum of selected exemplars vectors\n",
        "                mu_p = (vectors + S) / k                        # [n, feature_dim] sum to all vectors\n",
        "                i = np.argmin(np.sqrt(np.sum((class_mean - mu_p) ** 2, axis=1)))\n",
        "                selected_exemplars.append(np.array(data[i]))    # New object to avoid passing by inference\n",
        "                exemplar_vectors.append(np.array(vectors[i]))   # New object to avoid passing by inference\n",
        "\n",
        "                vectors = np.delete(vectors, i, axis=0)         # Remove it to avoid duplicative selection\n",
        "                data = np.delete(data, i, axis=0)               # Remove it to avoid duplicative selection\n",
        "\n",
        "            selected_exemplars = np.array(selected_exemplars)\n",
        "            exemplar_targets = np.full(m, class_idx)\n",
        "            self._data_memory = (\n",
        "                np.concatenate((self._data_memory, selected_exemplars))\n",
        "                if len(self._data_memory) != 0\n",
        "                else selected_exemplars\n",
        "            )\n",
        "            self._targets_memory = (\n",
        "                np.concatenate((self._targets_memory, exemplar_targets))\n",
        "                if len(self._targets_memory) != 0\n",
        "                else exemplar_targets\n",
        "            )\n",
        "\n",
        "            # Exemplar mean\n",
        "            idx_dataset = data_manager.get_dataset([],source=\"train\",mode=\"test\",appendent=(selected_exemplars, exemplar_targets))\n",
        "            idx_loader = DataLoader(idx_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "            vectors, _ = self._extract_vectors(idx_loader)\n",
        "            vectors = (vectors.T / (np.linalg.norm(vectors.T, axis=0) + EPSILON)).T\n",
        "            mean = np.mean(vectors, axis=0)\n",
        "            mean = mean / np.linalg.norm(mean)\n",
        "            self._class_means[class_idx, :] = mean\n",
        "\n",
        "    def build_rehearsal_memory(self, data_manager, per_class):\n",
        "        self._reduce_exemplar(data_manager, per_class)\n",
        "        self._construct_exemplar(data_manager, per_class)\n",
        "\n",
        "    def eval_task(self):\n",
        "        y_pred, y_true = self._eval_cnn(self.test_loader)\n",
        "        cnn_accy = self._evaluate(y_pred, y_true)\n",
        "        return cnn_accy\n",
        "\n",
        "    def _eval_cnn(self, loader):\n",
        "        self._network.eval()\n",
        "        y_pred, y_true = [], []\n",
        "        for _, (_, inputs, targets) in enumerate(loader):\n",
        "            inputs = inputs.to(self._device)\n",
        "            with torch.no_grad():\n",
        "                outputs = self._network(inputs)[\"logits\"]\n",
        "            predicts = torch.topk(outputs, k=self.topk, dim=1, largest=True, sorted=True)[1]  # [bs, topk]\n",
        "            y_pred.append(predicts.cpu().numpy())\n",
        "            y_true.append(targets.cpu().numpy())\n",
        "        return np.concatenate(y_pred), np.concatenate(y_true)  # [N, topk]\n",
        "\n",
        "    def _evaluate(self, y_pred, y_true):\n",
        "        ret = {}\n",
        "        grouped = accuracy(y_pred.T[0], y_true, self._known_classes)\n",
        "        ret[\"grouped\"] = grouped\n",
        "        ret[\"top1\"] = grouped[\"total\"]\n",
        "        ret[\"top{}\".format(self.topk)] = np.around((y_pred.T == np.tile(y_true, (self.topk, 1))).sum() * 100 / len(y_true),decimals=2)\n",
        "        return ret\n",
        "\n",
        "    def _extract_vectors(self, loader):\n",
        "        self._network.eval()\n",
        "        vectors, targets = [], []\n",
        "        for _, _inputs, _targets in loader:\n",
        "            _targets = _targets.numpy()\n",
        "            if isinstance(self._network, nn.DataParallel):\n",
        "                _vectors = tensor2numpy(self._network.module.extract_vector(_inputs.to(self._device)))\n",
        "            else:\n",
        "                _vectors = tensor2numpy(self._network.extract_vector(_inputs.to(self._device)))\n",
        "            vectors.append(_vectors)\n",
        "            targets.append(_targets)\n",
        "        return np.concatenate(vectors), np.concatenate(targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "O4bUZEx2uqGl"
      },
      "outputs": [],
      "source": [
        "# Incremental learning을 위한 class\n",
        "# DERBase를 상속\n",
        "class DER(DERBase):\n",
        "    def __init__(self, args):\n",
        "        super().__init__(args)\n",
        "\n",
        "    def incremental_train(self, data_manager):\n",
        "        self._cur_task += 1\n",
        "        self._total_classes = self._known_classes + data_manager.get_task_size(self._cur_task)\n",
        "        self._network.update_fc(self._total_classes)\n",
        "        if self._cur_task > 0:\n",
        "            for i in range(self._cur_task):\n",
        "                # 이전 task에서 학습한 convnet은 freeze\n",
        "                for p in self._network.convnets[i].parameters():\n",
        "                    p.requires_grad = False\n",
        "        train_dataset = data_manager.get_dataset(np.arange(self._known_classes, self._total_classes),\n",
        "                                                 source=\"train\", mode=\"train\", appendent=self._get_memory())\n",
        "        self.train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
        "        test_dataset = data_manager.get_dataset(np.arange(0, self._total_classes), source=\"test\", mode=\"test\")\n",
        "        self.test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
        "        self._train(self.train_loader, self.test_loader)\n",
        "        self.build_rehearsal_memory(data_manager, self.samples_per_class)\n",
        "\n",
        "    # train mode로 전환\n",
        "    def train(self):\n",
        "        self._network.train()\n",
        "        self._network.convnets[-1].train()\n",
        "        if self._cur_task >= 1:\n",
        "            for i in range(self._cur_task):\n",
        "                self._network.convnets[i].eval()\n",
        "\n",
        "    def _train(self, train_loader, test_loader):\n",
        "        self._network.to(self._device)\n",
        "        optimizer = optim.SGD(filter(lambda p: p.requires_grad, self._network.parameters()), momentum=0.9, lr=init_lr, weight_decay=init_weight_decay)\n",
        "        scheduler = optim.lr_scheduler.MultiStepLR(optimizer=optimizer, milestones=init_milestones, gamma=init_lr_decay)\n",
        "        if self._cur_task == 0:\n",
        "            self.init_train(self._network, train_loader, test_loader, optimizer, scheduler)\n",
        "        else:\n",
        "            self.update_representation(self._network, train_loader, test_loader, optimizer, scheduler)\n",
        "            self._network.weight_align(self._total_classes - self._known_classes)\n",
        "\n",
        "    # init task의 학습 : incremental learning이 아닌 일반적인 모델 학습과 동일\n",
        "    def init_train(self, network, train_loader, test_loader, optimizer, scheduler):\n",
        "        prog_bar = tqdm(range(init_epoch))\n",
        "        for _, epoch in enumerate(prog_bar):\n",
        "            self.train()\n",
        "            losses = 0.0\n",
        "            correct, total = 0, 0\n",
        "            for i, (_, inputs, targets) in enumerate(train_loader):\n",
        "                inputs, targets = inputs.to(self._device), targets.to(self._device)\n",
        "                logits = self._network(inputs)[\"logits\"]\n",
        "                loss = F.cross_entropy(logits, targets)\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                losses += loss.item()\n",
        "                _, preds = torch.max(logits, dim=1)\n",
        "                correct += preds.eq(targets.expand_as(preds)).cpu().sum()\n",
        "                total += len(targets)\n",
        "\n",
        "            scheduler.step()\n",
        "            train_acc = np.around(tensor2numpy(correct) * 100 / total, decimals=2)\n",
        "\n",
        "            if epoch % 5 == 0:\n",
        "                test_acc = self._compute_accuracy(self._network, test_loader)\n",
        "                info = \"Task {}, Epoch {}/{} => Loss {:.3f}, Train_accy {:.2f}, Test_accy {:.2f}\".format(self._cur_task, epoch + 1, init_epoch,\n",
        "                                                                                                        losses / len(train_loader), train_acc, test_acc)\n",
        "            prog_bar.set_description(info)\n",
        "\n",
        "    # init task 이후의 학습 : auxiliary loss를 추가\n",
        "    def update_representation(self, network, train_loader, test_loader, optimizer, scheduler):\n",
        "        prog_bar = tqdm(range(epochs))\n",
        "        for _, epoch in enumerate(prog_bar):\n",
        "            self.train()\n",
        "            losses = 0.0\n",
        "            losses_clf = 0.0\n",
        "            losses_aux = 0.0\n",
        "            correct, total = 0, 0\n",
        "            for i, (_, inputs, targets) in enumerate(train_loader):\n",
        "                inputs, targets = inputs.to(self._device), targets.to(self._device)\n",
        "                outputs = self._network(inputs)\n",
        "                logits, aux_logits = outputs[\"logits\"], outputs[\"aux_logits\"]\n",
        "                loss_clf = F.cross_entropy(logits, targets)\n",
        "                aux_targets = targets.clone()\n",
        "                aux_targets = torch.where(\n",
        "                    aux_targets - self._known_classes + 1 > 0,\n",
        "                    aux_targets - self._known_classes + 1, 0)\n",
        "                loss_aux = F.cross_entropy(aux_logits, aux_targets)\n",
        "                loss = loss_clf + loss_aux\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                losses += loss.item()\n",
        "                losses_aux += loss_aux.item()\n",
        "                losses_clf += loss_clf.item()\n",
        "                _, preds = torch.max(logits, dim=1)\n",
        "                correct += preds.eq(targets.expand_as(preds)).cpu().sum()\n",
        "                total += len(targets)\n",
        "\n",
        "            scheduler.step()\n",
        "            train_acc = np.around(tensor2numpy(correct) * 100 / total, decimals=2)\n",
        "            if epoch % 5 == 0:\n",
        "                test_acc = self._compute_accuracy(self._network, test_loader)\n",
        "                info = \"Task {}, Epoch {}/{} => Loss {:.3f}, Loss_clf {:.3f}, Loss_aux {:.3f}, Train_accy {:.2f}, Test_accy {:.2f}\".format(\n",
        "                    self._cur_task, epoch + 1, epochs,\n",
        "                    losses / len(train_loader), losses_clf / len(train_loader),losses_aux / len(train_loader),\n",
        "                    train_acc, test_acc)\n",
        "            prog_bar.set_description(info)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementation"
      ],
      "metadata": {
        "id": "0pl9RkzuT6dQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "8GB1qYkcsFqH"
      },
      "outputs": [],
      "source": [
        "# init_cls는 처음에 학습할 classes의 개수\n",
        "# increment는 두번째 task 이후 매 task마다 추가적으로 학습할 classes의 개수\n",
        "args = {\"memory_size\": 2000, \"memory_per_class\": 20, \"fixed_memory\": False,\n",
        "        \"init_cls\": 25,\"increment\": 25, \"device\": [\"cuda:0\"], \"seed\": [1993]}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6wKO7x7gsFqH",
        "outputId": "0c1f41df-8a4f-4302-b387-95057064619b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data/cifar-100-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 169001437/169001437 [00:07<00:00, 23746243.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-100-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "# 실행\n",
        "model = DER(args)\n",
        "data_manager = DataManager(args[\"seed\"], args[\"init_cls\"], args[\"increment\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P7rCYHKTsFqI",
        "outputId": "914a54d3-849f-49b6-dd74-d7754bc8d47b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All params: 0\n",
            "Trainable params: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Task 0, Epoch 201/201 => Loss 0.015, Train_accy 99.86, Test_accy 85.04: 100%|██████████| 201/201 [31:11<00:00,  9.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reducing exemplars...(80 per classes)\n",
            "Constructing exemplars...(80 per classes)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exemplar size: 2000\n",
            "CNN: {'total': 85.04, '00-09': 86.5, '10-19': 83.6, '20-29': 85.0, 'old': 0, 'new': 85.04}\n",
            "top1 curve: [85.04]\n",
            "top5 curve: [97.56]\n",
            "\n",
            "Average Accuracy : 85.04\n",
            "Average Accuracy : 85.04\n",
            "All params: 467469\n",
            "Trainable params: 467469\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Task 1, Epoch 171/171 => Loss 0.041, Loss_clf 0.022, Loss_aux 0.020, Train_accy 99.79, Test_accy 71.62: 100%|██████████| 171/171 [32:47<00:00, 11.51s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "alignweights,gamma= tensor(0.7513, device='cuda:0')\n",
            "Reducing exemplars...(40 per classes)\n",
            "Constructing exemplars...(40 per classes)\n",
            "Exemplar size: 2000\n",
            "CNN: {'total': 74.56, '00-09': 75.8, '10-19': 70.1, '20-29': 75.1, '30-39': 75.5, '40-49': 76.3, 'old': 74.08, 'new': 75.04}\n",
            "top1 curve: [85.04, 74.56]\n",
            "top5 curve: [97.56, 93.92]\n",
            "\n",
            "Average Accuracy : 79.80000000000001\n",
            "Average Accuracy : 79.80000000000001\n",
            "All params: 936448\n",
            "Trainable params: 472294\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Task 2, Epoch 171/171 => Loss 0.046, Loss_clf 0.025, Loss_aux 0.022, Train_accy 99.86, Test_accy 61.53: 100%|██████████| 171/171 [36:39<00:00, 12.86s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "alignweights,gamma= tensor(0.6516, device='cuda:0')\n",
            "Reducing exemplars...(26 per classes)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Constructing exemplars...(26 per classes)\n",
            "Exemplar size: 1950\n",
            "CNN: {'total': 68.48, '00-09': 69.0, '10-19': 64.0, '20-29': 64.5, '30-39': 68.4, '40-49': 72.1, '50-59': 76.4, '60-69': 63.7, '70-79': 71.0, 'old': 67.6, 'new': 70.24}\n",
            "top1 curve: [85.04, 74.56, 68.48]\n",
            "top5 curve: [97.56, 93.92, 90.73]\n",
            "\n",
            "Average Accuracy : 76.02666666666669\n",
            "Average Accuracy : 76.02666666666669\n",
            "All params: 1408627\n",
            "Trainable params: 480319\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Task 3, Epoch 171/171 => Loss 0.045, Loss_clf 0.026, Loss_aux 0.018, Train_accy 99.85, Test_accy 54.79: 100%|██████████| 171/171 [42:39<00:00, 14.97s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "alignweights,gamma= tensor(0.6183, device='cuda:0')\n",
            "Reducing exemplars...(20 per classes)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Constructing exemplars...(20 per classes)\n",
            "Exemplar size: 2000\n",
            "CNN: {'total': 64.86, '00-09': 63.6, '10-19': 58.6, '20-29': 53.7, '30-39': 56.5, '40-49': 63.2, '50-59': 77.5, '60-69': 62.7, '70-79': 68.6, '80-89': 71.6, '90-99': 72.6, 'old': 62.84, 'new': 70.92}\n",
            "top1 curve: [85.04, 74.56, 68.48, 64.86]\n",
            "top5 curve: [97.56, 93.92, 90.73, 88.79]\n",
            "\n",
            "Average Accuracy : 73.23500000000001\n",
            "Average Accuracy : 73.23500000000001\n"
          ]
        }
      ],
      "source": [
        "cnn_curve = {\"top1\": [], \"top5\": []}\n",
        "for task in range(data_manager.nb_tasks):\n",
        "    print(\"All params: {}\".format(count_parameters(model._network)))\n",
        "    print(\"Trainable params: {}\".format(count_parameters(model._network, True)))\n",
        "    model.incremental_train(data_manager)\n",
        "    cnn_accy = model.eval_task()\n",
        "    model.after_task()\n",
        "    print(\"CNN: {}\".format(cnn_accy[\"grouped\"]))\n",
        "\n",
        "    cnn_curve[\"top1\"].append(cnn_accy[\"top1\"])\n",
        "    cnn_curve[\"top5\"].append(cnn_accy[\"top5\"])\n",
        "\n",
        "    print(\"top1 curve: {}\".format(cnn_curve[\"top1\"]))\n",
        "    print(\"top5 curve: {}\\n\".format(cnn_curve[\"top5\"]))\n",
        "    print('Average Accuracy :', sum(cnn_curve[\"top1\"])/len(cnn_curve[\"top1\"]))\n",
        "    print(\"Average Accuracy : {}\".format(sum(cnn_curve[\"top1\"])/len(cnn_curve[\"top1\"])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "1HbNtkMesFqJ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}